"""
AI Content Generation Lambda function.
Generates SEO-optimized pages using Amazon Bedrock LLMs.
"""

import json
import os
import logging
from typing import Dict, Any, List, Optional
import boto3
from botocore.exceptions import ClientError

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Add common modules to path
from schemas import Business, PageSpec, PipelineStatus, GenerationTrace
from s3_utils import S3Manager
from bedrock_client import BedrockClient
from prompts import get_generation_prompt, get_category_context


def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Lambda handler for AI content generation.

    Args:
        event: Lambda event (Step Functions input)
        context: Lambda context

    Returns:
        Dictionary with generation results
    """
    logger.info(f"Starting AI content generation with event: {json.dumps(event, default=str)}")

    try:
        # Initialize clients
        s3_manager = S3Manager(region_name=os.environ.get('AWS_REGION', 'us-east-1'))
        bedrock_client = BedrockClient(region_name=os.environ.get('BEDROCK_REGION', 'us-east-1'))

        # Get bucket names from environment
        processed_bucket = os.environ['PROCESSED_BUCKET']

        # Extract parameters from event
        execution_id = event.get('execution_id', context.aws_request_id)
        input_file = event.get('output_file')  # From previous step

        if not input_file:
            raise Exception("No input file specified from previous step")

        # Initialize pipeline status
        pipeline_status = PipelineStatus(
            execution_id=execution_id,
            stage='agent_generate',
            total_businesses=0
        )

        # Download cleaned business data
        logger.info(f"Downloading cleaned data: s3://{processed_bucket}/{input_file}")
        business_data = s3_manager.download_json(processed_bucket, input_file)

        if not business_data:
            error_msg = f"Failed to download business data from {input_file}"
            logger.error(error_msg)
            pipeline_status.errors.append(error_msg)
            pipeline_status.stage = 'failed'
            s3_manager.save_pipeline_status(processed_bucket, pipeline_status)
            raise Exception(error_msg)

        businesses = business_data.get('businesses', [])
        pipeline_status.total_businesses = len(businesses)

        logger.info(f"Processing {len(businesses)} businesses for content generation")

        # Generate content for each business
        generated_pages = []
        generation_traces = []
        successful_generations = 0
        failed_generations = 0

        for idx, business_dict in enumerate(businesses):
            try:
                # Create Business object
                business = Business(**business_dict)
                logger.info(f"Generating content for: {business.name} ({business.category})")

                # Get category-specific context
                category_context = get_category_context(business.category)

                # Find related businesses for internal linking (optional)
                related_businesses = find_related_businesses(business, businesses[:10])

                # Generate content prompt
                prompt = get_generation_prompt(business, related_businesses)

                # Generate content using Bedrock
                generated_content, trace = bedrock_client.generate_content(
                    prompt=prompt,
                    business_id=business.business_id,
                    model_name='claude-3-haiku'
                )

                if generated_content:
                    try:
                        # Skip validation and use the generated content directly
                        # page_spec = PageSpec(**generated_content)  # Disabled validation

                        # Store the generated page without validation
                        generated_pages.append({
                            'business_id': business.business_id,
                            'page_spec': generated_content,  # Use raw content without validation
                            'generation_successful': True
                        })
                        successful_generations += 1

                        logger.info(f"Successfully generated content for {business.name}")

                    except Exception as validation_error:
                        error_msg = f"Generated content validation failed for {business.business_id}: {str(validation_error)}"
                        trace.errors.append(error_msg)
                        logger.error(error_msg)

                        generated_pages.append({
                            'business_id': business.business_id,
                            'page_spec': None,
                            'generation_successful': False,
                            'error': error_msg
                        })
                        failed_generations += 1
                else:
                    error_msg = f"Content generation failed for {business.business_id}"
                    logger.error(error_msg)

                    generated_pages.append({
                        'business_id': business.business_id,
                        'page_spec': None,
                        'generation_successful': False,
                        'error': error_msg
                    })
                    failed_generations += 1

                # Store generation trace
                generation_traces.append(trace.dict())

            except Exception as e:
                error_msg = f"Error processing business {idx + 1}: {str(e)}"
                logger.error(error_msg)
                pipeline_status.errors.append(error_msg)
                failed_generations += 1

        # Update pipeline status
        pipeline_status.processed_businesses = successful_generations

        # Check if all generations failed - this should be treated as a pipeline failure
        if successful_generations == 0 and len(businesses) > 0:
            error_msg = f"All {len(businesses)} content generation attempts failed"
            logger.error(error_msg)
            pipeline_status.errors.append(error_msg)
            pipeline_status.stage = 'failed'

            # Still save the data for debugging purposes
            output_key = f"content/generated_{execution_id}.json"
            output_data = {
                'execution_id': execution_id,
                'source_file': input_file,
                'total_businesses': len(businesses),
                'successful_generations': successful_generations,
                'failed_generations': failed_generations,
                'generated_pages': generated_pages,
                'generation_traces': generation_traces
            }
            s3_manager.upload_json(processed_bucket, output_key, output_data)

            try:
                s3_manager.save_pipeline_status(processed_bucket, pipeline_status)
            except Exception as e:
                logger.warning(f"Failed to save pipeline status: {str(e)}")

            raise Exception(error_msg)

        # Save generated content
        output_key = f"content/generated_{execution_id}.json"
        output_data = {
            'execution_id': execution_id,
            'source_file': input_file,
            'total_businesses': len(businesses),
            'successful_generations': successful_generations,
            'failed_generations': failed_generations,
            'generated_pages': generated_pages,
            'generation_traces': generation_traces
        }

        if not s3_manager.upload_json(processed_bucket, output_key, output_data):
            error_msg = "Failed to save generated content"
            logger.error(error_msg)
            pipeline_status.errors.append(error_msg)
            pipeline_status.stage = 'failed'
            s3_manager.save_pipeline_status(processed_bucket, pipeline_status)
            raise Exception(error_msg)

        # Save pipeline status
        pipeline_status.stage = 'completed'
        try:
            s3_manager.save_pipeline_status(processed_bucket, pipeline_status)
            logger.info("Pipeline status saved successfully")
        except Exception as e:
            logger.warning(f"Failed to save pipeline status: {str(e)} - continuing anyway")

        # Prepare response
        response = {
            'statusCode': 200,
            'execution_id': execution_id,
            'total_businesses': len(businesses),
            'successful_generations': successful_generations,
            'failed_generations': failed_generations,
            'output_file': output_key
        }

        logger.info(f"Content generation completed: {successful_generations}/{len(businesses)} successful")
        return response

    except Exception as e:
        error_msg = f"Content generation failed: {str(e)}"
        logger.error(error_msg)

        # Try to update pipeline status
        try:
            if 'pipeline_status' in locals():
                pipeline_status.stage = 'failed'
                pipeline_status.errors.append(error_msg)
                s3_manager.save_pipeline_status(processed_bucket, pipeline_status)
        except Exception as status_error:
            logger.error(f"Failed to save pipeline status: {str(status_error)}")

        return {
            'statusCode': 500,
            'error': error_msg,
            'execution_id': event.get('execution_id', context.aws_request_id),
            'output_file': None,
            'total_businesses': 0,
            'successful_generations': 0,
            'failed_generations': 0
        }


def find_related_businesses(target_business: Business, all_businesses: List[Dict[str, Any]]) -> List[Business]:
    """
    Find businesses related to the target business for internal linking.

    Args:
        target_business: The business to find relations for
        all_businesses: List of all business dictionaries

    Returns:
        List of related Business objects
    """
    related = []
    target_category = target_business.category.lower()
    target_city = target_business.city.lower()

    for business_dict in all_businesses:
        try:
            business = Business(**business_dict)

            # Skip self
            if business.business_id == target_business.business_id:
                continue

            # Prioritize same city, different category
            if (business.city.lower() == target_city and
                business.category.lower() != target_category):
                related.append(business)

            # Add same category, different city as secondary
            elif (business.category.lower() == target_category and
                  business.city.lower() != target_city):
                related.append(business)

        except Exception as e:
            logger.warning(f"Error processing related business: {str(e)}")
            continue

    # Return up to 3 related businesses
    return related[:3]


def estimate_generation_cost(businesses: List[Dict[str, Any]]) -> float:
    """
    Estimate the cost of generating content for all businesses.

    Args:
        businesses: List of business dictionaries

    Returns:
        Estimated cost in USD
    """
    # Rough estimates based on typical prompt/response sizes
    avg_input_tokens = 800  # Business data + prompt
    avg_output_tokens = 1500  # Generated page content

    total_cost = 0.0
    for business in businesses:
        # Use Claude-3 Sonnet pricing
        cost = BedrockClient().estimate_cost(avg_input_tokens, avg_output_tokens, 'claude-3-sonnet')
        total_cost += cost

    return round(total_cost, 4)