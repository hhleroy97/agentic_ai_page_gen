# robots.txt for Local Business Directory
# Generated by Agentic Local SEO Content Factory

User-agent: *
Allow: /

# Preferred crawling
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

# Block common unwanted bots
User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: DotBot
Disallow: /

# Sitemap location
Sitemap: {{WEBSITE_URL}}/sitemap.xml

# Crawl delay for politeness
Crawl-delay: 1

# Disallow specific paths (if any)
Disallow: /admin/
Disallow: /private/
Disallow: /_internal/
Disallow: /temp/
Disallow: /*.json$
Disallow: /analytics.js

# Allow important files
Allow: /robots.txt
Allow: /sitemap.xml
Allow: /favicon.ico
Allow: /manifest.json
Allow: /humans.txt

# SEO optimization notes:
# - This file helps search engines understand how to crawl the site
# - Sitemap URL will be populated during deployment
# - Crawl delays help prevent server overload
# - Generated: {{GENERATION_DATE}}